{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load necessary packages and iscam_analysis software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('module://ipympl.backend_nbagg')\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append('../iscam_analysis/')\n",
    "from iscam_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load iSCAM data into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to read a list of iscam intensity files that are going to be combined into one dataset\n",
    "# Select the path where the data files are stored\n",
    "path = '../example_data'\n",
    "# Choose a pattern to select the files to be loaded with\n",
    "# A '*' can be any chain of characters. ''*.h5' will\n",
    "# therefore match all files ending with '.h5'.\n",
    "pattern = '*fitted*.h5'\n",
    "# Load files recursively which obey the pattern\n",
    "recursive = True\n",
    "# Set the calbration to convert between intensities and molecular weight in kDa\n",
    "mw_intensity_line_pars = [2.59682927e-05, 6.35853659e-05]  # [slope, intercept]\n",
    "# Choose a meaningful key/name for the dataset to be created\n",
    "key = 'NAME_OF_DATASET'\n",
    "\n",
    "# Load the iscam data and create a dataset\n",
    "filelist = get_iscam_h5py_filelist(path, pattern=pattern, recursive=recursive)\n",
    "dataset = create_iscam_dataset(filelist, mw_intensity_line_pars=mw_intensity_line_pars, name=key)\n",
    "dataset['info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and plot iSCAM data of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fitting and plotting parameters\n",
    "# Set initial start values for gauss peak centers\n",
    "centers_init = [90, 180, 360, 540, 720]\n",
    "# Set the range (center +/- center_pm) a center can be varied during fitting\n",
    "center_pm = 50  # kDa\n",
    "# Set the initial full width at half maximum of the gaussians\n",
    "fwhm = mw = 50  # kDa\n",
    "# Fit CDF or histogram\n",
    "cdf = False\n",
    "# Set the parameters of the linear function the sigmas of the gaussians\n",
    "# should be constrained to depending on the centers [slope, intercept]:\n",
    "# sigma = slope * center + intercept\n",
    "# A value of None disables the constrain of sigma.\n",
    "sigma_center_line_pars = None  # [slope, intercept]\n",
    "# Set range of weight events to be used for fitting (min, max)\n",
    "mw_range = (0, 1000)  # kDa\n",
    "# Set number of bins the mw_range will be split into\n",
    "bins = 100\n",
    "# Use means of MWs of datapoints in bins\n",
    "# instead of center of bins for fitting\n",
    "use_x_means= False\n",
    "# Use weights (uncertainties) of number of datapoints\n",
    "# of the bins for fitting\n",
    "use_y_weights = False\n",
    "# Plot black lines at fitted centers\n",
    "centers = True\n",
    "# Plot CDF of protomers\n",
    "cdf_protomers = True\n",
    "# Plot CDF of monomers\n",
    "cdf_monomers = False\n",
    "# Plot CDF of fit\n",
    "cdf_fit = False\n",
    "# Plot individual gaussian components or the sum of all gaussians\n",
    "components = True\n",
    "# Select the range taht should be plotted\n",
    "plot_range = (0, 1000)  # kDa\n",
    "# Set the molecular weight the xticks should be separated\n",
    "mw = 25  # kDa\n",
    "# Select the xticks that should have a MW label\n",
    "labeled_xticks = [0, 20, 40]  # number of mws\n",
    "# Set path to where the figure should be saved. See\n",
    "# matplotlib documentation for supported file types.\n",
    "# A value of None disables the saving of the figure.\n",
    "figpath = '../hist_fit.png'\n",
    "\n",
    "# Do the fitting and plotting\n",
    "result, fig_ax = fit_and_plot_iscam(dataset, centers_init, center_pm=center_pm, fwhm=fwhm, cdf=cdf,\n",
    "                                    sigma_center_line_pars=sigma_center_line_pars, mw_range=mw_range,\n",
    "                                    bins=bins, use_x_means=use_x_means, use_y_weights=use_y_weights,\n",
    "                                    centers=centers, cdf_protomers=cdf_protomers,\n",
    "                                    cdf_monomers=cdf_monomers, cdf_fit=cdf_fit,\n",
    "                                    components=components, plot_range=plot_range, mw=mw,\n",
    "                                    labeled_xticks=labeled_xticks, figpath=figpath)\n",
    "\n",
    "# Convert fit result to more convenient dictionaries/numpy ndarrays\n",
    "#fit_params = get_fit_params(result.params, verbose=True)\n",
    "#fit_values = get_fit_values(fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve a fit result from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best fit result (according to the Akaike criterion) from a dataset\n",
    "result_key, result_params, fit_params = get_best_result(dataset)\n",
    "\n",
    "# Get settings, fit_report, fit_params and result_params from best fit result\n",
    "settings = dataset['results'][result_key]['settings']\n",
    "fit_report = dataset['results'][result_key]['fit_report']\n",
    "fit_params = dataset['results'][result_key]['fit_params']\n",
    "result_params = dataset['results'][result_key]['result_params']\n",
    "\n",
    "# Print fit report, content of fit_params, and show result_params\n",
    "print('FIT REPORT:\\n', fit_report, '\\n')\n",
    "print('FIT PARAMS:\\n', fit_params, '\\n')\n",
    "print('RESULT PARAMS:')\n",
    "result_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a dataset in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the previously created dataset in a fresh dictionary with the key 'NAME_OF_DATASET':\n",
    "datasets = {}\n",
    "key = 'NAME_OF_DATASET'\n",
    "datasets[key] = dataset\n",
    "\n",
    "# Save dictionary to disk\n",
    "with open('./datasets.pkl','wb') as f:\n",
    "    pickle.dump(datasets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a dataset from a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary from disk\n",
    "with open('./datasets.pkl','rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "\n",
    "# Get dataset from dictionary with the key 'NAME_OF_DATASET':\n",
    "key = 'NAME_OF_DATASET'    \n",
    "dataset = datasets[key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
